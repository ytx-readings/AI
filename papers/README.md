# AI Papers

## [LLM](./LLM/)

* [CMMLU: Measuring massive multitask language understanding in Chinese](./LLM/CMMLU.pdf)
* [CodeFuse-13B: A Pretrained Multi-lingual Code Large Language Model](./LLM/CodeFuse-13B.pdf)
* [CROSSCODEEVAL: A Diverse and Multilingual Benchmark for Cross-File Code Completion](./LLM/CrossCodeEval.pdf)
* [CRUXEval-X: A Benchmark for Multilingual Code Reasoning, Understanding and Execution](./LLM/CRUXEval-X.pdf)
* [CRUXEval: A Benchmark for Code Reasoning, Understanding and Execution](./LLM/CRUXEval.pdf)
* [DeepSeek-Coder: When the Large Language Model Meets Programming &ndash; The Rise of Code Intelligence](./LLM/DeepSeek-Coder.pdf)
* [DemoCraft: Using In-Context Learning to Improve Code Generation in Large Language Models](./LLM/DemoCraft.pdf)
* [Efficient Training of Language Models to Fill in the Middle](./LLM/Efficient%20Training%20of%20Language%20Models%20to%20Fill%20in%20the%20Middle.pdf)
* [Evaluating Large Language Models Trained on Code](./LLM/Evaluating%20Large%20Language%20Models%20Trained%20on%20Code.pdf)
* [Evaluation of LLMs on Syntax-Aware Code Fill-in-the-Middle Tasks](./LLM/Evaluation%20of%20LLMs%20on%20Syntax-Aware%20Code%20Fill-in-the-Middle%20Tasks.pdf)
* [InterTrans: Leveraging Transitive Intermediate Translations to Enhance LLM-based Code Translation](./LLM/InterTrans.pdf)
* [LLM-Eval: Unified Multi-Dimensional Automatic Evaluation for Open-Domain Conversations with Large Language Models](./LLM/LLM-Eval.pdf)
* [McEval: Massively Multilingual Code Evaluation](./LLM/McEval.pdf)
* [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](./LLM/Megatron-LM.pdf)
* [Qwen2.5-Coder Technical Report](./LLM/Qwen2.5-Coder%20Technical%20Report.pdf)
* [SGLang: Efficient Execution of Structured Language Model Programs](./LLM/SGLang.pdf)

## [Neural Network](./neural%20network/)

* [Intention Recognition with Recurrent Neural Networks for Dynamic Human-Robot Collaboration](./neural%20network/Intention_Recognition_with_Recurrent_Neural_Networks_for_Dynamic_Human-Robot_Collaboration.pdf)

## [NLP](./NLP/)

* [Are Pre-trained Transformers Robust in Intent Classification? A Missing Ingredient in Evaluation of Out-of-Scope Intent Detection](./NLP/Are%20Pre-trained%20Transformers%20Robust%20in%20Intent%20Classification.pdf)
* [Balancing Accuracy and Efficiency in Multi-Turn Intent Classification for LLM-Powered Dialog Systems in Production](./NLP/Balancing%20Accuracy%20and%20Efficiency%20in%20Multi-Turn%20Intent%20Classification%20for%20LLM-Powered%20Dialog%20Systems%20in%20Production.pdf)
* [Continual Few-shot Intent Detection](./NLP/Continual%20Few-shot%20Intent%20Detection.pdf)
* [Dual-Objective Fine-Tuning of BERT for Entity Matching](./NLP/Dual-Objective%20Fine-Tuning%20of%20BERT%20for%20Entity%20Matching.pdf)
* [Few-Shot Contrastive Learning-Based Multi-Round Dialog Intent Classification Method](./NLP/Few‐Shot%20Contrastive%20Learning‐Based%20Multi‐Round%20Dialogue%20Intent%20Classification%20Method.pdf)
* [Few-Shot Intent Detection via Contrastive Pre-Training and Fine-Tuning](./NLP/Few-Shot%20Intent%20Detection%20via%20Contrastive%20Pre-Training%20and%20Fine-Tuning.pdf)
* [Infusing Context and Knowledge Awareness in Multi-turn Dialog Understanding](./NLP/Infusing%20Context%20and%20Knowledge%20Awareness%20in%20Multi-turn%20Dialog%20Understanding.pdf)
* [Intent-Aware Dialogue Generation and Multi-Task Contrastive Learning for Multi-Turn Intent Classification](./NLP/Intent-Aware%20Dialogue%20Generation%20and%20Multi-Task%20Contrastive%20Learning%20for%20Multi-Turn%20Intent%20Classification.pdf)
* [Intent detection for task-oriented conversational agents: A comparative study of recurrent neural networks and transformer models](./NLP/Intent%20detection%20for%20task-oriented%20conversational%20agents%20A%20comparative%20study%20of%20recurrent%20neural%20networks%20and%20transformer%20models.pdf)
* [Joint Multiple Intent Detection and Slot Labeling for Goal-Oriented Dialog](./NLP/Joint%20Multiple%20Intent%20Detection%20and%20Slot%20Labeling%20for%20Goal-Oriented%20Dialog.pdf)
* [Knowledge Augmented BERT Mutual Network in Multi-turn Spoken Dialogues](./NLP/Knowledge%20Augmented%20BERT%20Mutual%20Network%20in%20Multi-turn%20Spoken%20Dialogues.pdf)
* [LARA: Lingustic-Adaptive Retrieval-Augmentation for Multi-Turn Intent Classification](./NLP/LARA%20Linguistic-Adaptive%20Retrieval-Augmentation%20for%20Multi-Turn%20Intent%20Classification.pdf)
* [MIDAS: Multi-level Intent, Domain, And Slot Knowledge Distillation for Multi-turn NLU](./NLP/MIDAS%20Multi-level%20Intent,%20Domain,%20And%20Slot%20Knowledge%20Distillation%20for%20Multi-turn%20NLU.pdf)
* [MIDLM: Multi-Intent Detection with Bidirectional Large Language Models](./NLP/MIDLM%20Multi-Intent%20Detection%20with%20Bidirectional%20Large%20Language%20Models.pdf)
* [MTSI-BERT: A Session-aware Knowledge-based Conversational Agent](./NLP/MTSI-BERT%20A%20Session-aware%20Knowledge-based%20Conversational%20Agent.pdf)
* [RoBERTa: A Robustly Optimized BERT Pretraining Approach](./NLP/RoBERTa%20A%20Robustly%20Optimized%20BERT%20Pretraining%20Approach.pdf)
* [SELF-ICL: Zero-Shot In-Context Learning with Self-Generated Demonstrations](./NLP/SELF-ICL%20Zero-Shot%20In-Context%20Learning%20with%20Self-Generated%20Demonstrations.pdf)
* [基于多任务蒸馏的意图识别和槽位填充](./NLP/基于多任务蒸馏的意图识别和槽位填充.pdf)

## [Transformer Model](./transformer%20model/)

* [Attention is All You Need](./transformer%20model/Attention%20is%20All%20You%20Need.pdf) ([Reading Notes](../reading%20notes/papers/Attention%20is%20All%20You%20Need.md))
* [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](./transformer%20model/BERT%20Pre-training%20of%20Deep%20Bidirectional%20Transformers%20for%20Language%20Understanding.pdf)
* [Transformed-based Map Matching Model with Limited Ground-Truth Data using Transfer-Learning Approach](./transformer%20model/Transformer-based%20Map%20Matching%20Model%20with%20Limited%20Ground-Truth%20Data%20using%20Transfer-Learning%20Approach.pdf)
* [Transformer++](./transformer%20model/Transformer++.pdf)
* [Transformers: State-of-the-Art Natural Language Processing](./transformer%20model/Transformers%20State-of-the-Art%20Natural%20Language%20Processing.pdf)